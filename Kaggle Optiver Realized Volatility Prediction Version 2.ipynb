{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data path\n",
    "\n",
    "path = '~/Datasets/Kaggle Optiver'\n",
    "\n",
    "list_order_book_file_train = glob.glob(f'{path}/book_train.parquet/*')\n",
    "list_order_book_file_test = glob.glob(f'{path}/book_test.parquet/*')\n",
    "\n",
    "list_order_trade_file_train = glob.glob(f'{path}/trade_train.parquet/*')\n",
    "list_order_trade_file_test = glob.glob(f'{path}/trade_test.parquet/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new features\n",
    "\n",
    " # Bid ask spread\n",
    "def bid_ask_spread1(df):\n",
    "    return df['ask_price1']/df['bid_price1'] - 1\n",
    "\n",
    "def bid_ask_spread2(df):\n",
    "    return df['ask_price2']/df['bid_price2'] - 1\n",
    "    \n",
    "def bid_ask_spread_all(df):\n",
    "    return (df['ask_price2'] + df['ask_price1']) / (df['bid_price2'] + df['bid_price1']) - 1\n",
    "\n",
    " # Weight average price\n",
    "def wap_1(df):\n",
    "    price = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\\\n",
    "    / (df['ask_size1'] + df['bid_size1'])\n",
    "    return price\n",
    "\n",
    "def wap_2(df):\n",
    "    price = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\\\n",
    "    / (df['ask_size2'] + df['bid_size2'])\n",
    "    return price\n",
    "\n",
    "def wap_3(df):\n",
    "    price = (df['bid_price1'] * df['ask_size2'] + df['ask_price2'] * df['bid_size1'])\\\n",
    "    / (df['ask_size2'] + df['bid_size1'])\n",
    "    return price\n",
    "\n",
    "def wap_4(df):\n",
    "    price = (df['bid_price2'] * df['ask_size1'] + df['ask_price1'] * df['bid_size2'])\\\n",
    "    / (df['ask_size1'] + df['bid_size2'])\n",
    "    return price\n",
    "\n",
    " # Log return\n",
    "def log_return(price):\n",
    "    return np.log(price).diff()\n",
    "\n",
    " # Volatility\n",
    "def volatility_single_step(log_return):\n",
    "     return np.sqrt(np.sum(np.square(log_return)))\n",
    "\n",
    "# Deal with book data\n",
    "\n",
    "def book_feature_eng(stock_data_all):\n",
    "    \n",
    "    stock_data_300 = stock_data_all[stock_data_all['seconds_in_bucket']<301]\n",
    "    stock_data_list = [stock_data_300, stock_data_all]\n",
    "    num_i = 0\n",
    "    \n",
    "    for stock_data in stock_data_list:\n",
    "        num_i += 1 \n",
    "        wap_list = [wap_1, wap_2, wap_3, wap_4]\n",
    "\n",
    "        for i in range(1,5):\n",
    "            stock_data.loc[:,f'WAP{i}'] = wap_list[i-1](stock_data)\n",
    "            stock_data.loc[:,f'log_return{i}'] = stock_data.groupby(['time_id'])[f'WAP{i}'].apply(log_return)\n",
    "            #stock_data = stock_data[~stock_data[f'log_return{i}'].isnull()]\n",
    "\n",
    "        stock_data.loc[:,'WAP_mean'] = stock_data.loc[:,['WAP1','WAP2','WAP3','WAP4']].mean(axis=1)\n",
    "\n",
    "        stock_data.loc[:,'log_return_mean'] = stock_data.groupby(['time_id'])['WAP_mean'].apply(log_return)\n",
    "        stock_data = stock_data[~stock_data[f'log_return{i}'].isnull()]\n",
    "\n",
    "        ba_spread_list = [bid_ask_spread1, bid_ask_spread2, bid_ask_spread_all]\n",
    "\n",
    "        for i in range(1,4):\n",
    "            stock_data.loc[:,f'ba_spread{i}'] = ba_spread_list[i-1](stock_data)\n",
    "\n",
    "        stock_data.loc[:,'total_volumn'] = stock_data['ask_size1'] + stock_data['ask_size2'] +\\\n",
    "                                           stock_data['bid_size1'] + stock_data['bid_size2']\n",
    "\n",
    "        f_eng_dict = {\n",
    "            'log_return1' : [volatility_single_step],\n",
    "            'log_return2' : [volatility_single_step],\n",
    "            'log_return3' : [volatility_single_step],\n",
    "            'log_return4' : [volatility_single_step],\n",
    "            'log_return_mean' : [volatility_single_step],\n",
    "            'ba_spread1' : [np.mean],\n",
    "            'ba_spread2' : [np.mean],\n",
    "            'total_volumn' : [np.mean],\n",
    "            'WAP_mean' : [np.mean]\n",
    "        }\n",
    "        \n",
    "        df_final = pd.DataFrame(stock_data.groupby(['time_id']).agg(f_eng_dict))\n",
    "        df_final = df_final.reset_index()\n",
    "        df_final.columns = [\n",
    "            'time_id',\n",
    "            'log_return1',\n",
    "            'log_return2',\n",
    "            'log_return3',\n",
    "            'log_return4',\n",
    "            'log_return_mean',\n",
    "            'ba_spread1',\n",
    "            'ba_spread2',\n",
    "            'total_volumn',\n",
    "            'WAP_mean'\n",
    "        ]\n",
    "        \n",
    "        if num_i == 1:\n",
    "            df_final_merge = df_final\n",
    "        else:\n",
    "            df_final_merge = df_final_merge.merge(df_final, on=['time_id'])\n",
    "    \n",
    "    return df_final_merge\n",
    "        \n",
    "def eng_all_book(file):   \n",
    "    \n",
    "    file_df = pd.read_parquet(file)\n",
    "    all_stock_vol = book_feature_eng(file_df)\n",
    "    all_stock_vol['stock_id'] = file.split('=')[1]\n",
    "    all_stock_vol['stock_id'] = all_stock_vol['stock_id'].astype('int32')\n",
    "    \n",
    "    return all_stock_vol\n",
    "\n",
    "# Deal with trade data\n",
    "\n",
    "def trade_feature_eng(stock_data_all):\n",
    "    \n",
    "    stock_data_300 = stock_data_all[stock_data_all['seconds_in_bucket']<301]\n",
    "    stock_data_list = [stock_data_300, stock_data_all]\n",
    "    num_i = 0\n",
    "    \n",
    "    for stock_data in stock_data_list:\n",
    "        num_i += 1 \n",
    "    \n",
    "        stock_data.loc[:,'log_return_trade'] = stock_data.groupby(['time_id'])['price'].apply(log_return)\n",
    "        stock_data = stock_data[~stock_data['log_return_trade'].isnull()]\n",
    "\n",
    "        f_eng_dict = {\n",
    "            'log_return_trade' : [volatility_single_step],\n",
    "            'size' : [np.mean],\n",
    "            'order_count' : [np.mean]\n",
    "            }\n",
    "\n",
    "        df_final = pd.DataFrame(stock_data.groupby(['time_id']).agg(f_eng_dict))\n",
    "        df_final = df_final.reset_index()\n",
    "        df_final.columns = [\n",
    "            'time_id',\n",
    "            'log_return_trade',\n",
    "            'size',\n",
    "            'order_count'\n",
    "        ]\n",
    "        \n",
    "        if num_i == 1:\n",
    "            df_final_merge = df_final\n",
    "        else:\n",
    "            df_final_merge = df_final_merge.merge(df_final, on=['time_id'])\n",
    "    \n",
    "    return df_final_merge\n",
    "\n",
    "def eng_all_trade(file):\n",
    "    \n",
    "    file_df = pd.read_parquet(file)\n",
    "    all_stock_vol = trade_feature_eng(file_df)\n",
    "    all_stock_vol['stock_id'] = file.split('=')[1]\n",
    "    all_stock_vol['stock_id'] = all_stock_vol['stock_id'].astype('int32')\n",
    "    \n",
    "    return all_stock_vol\n",
    "\n",
    "# Generate final features for all data\n",
    "\n",
    "def train_and_test_data(book_list, test_list):\n",
    "    \n",
    "    all_book = Parallel(n_jobs=-1)(delayed(eng_all_book)(file) for file in book_list)\n",
    "    all_book = pd.concat(all_book, ignore_index = True)\n",
    "    all_book = all_book.sort_values(by=['stock_id','time_id']).reset_index(drop=True)\n",
    "\n",
    "    all_trade = Parallel(n_jobs=-1)(delayed(eng_all_trade)(file) for file in test_list)\n",
    "    all_trade = pd.concat(all_trade, ignore_index = True)\n",
    "    all_trade = all_trade.sort_values(by=['stock_id','time_id']).reset_index(drop=True)\n",
    "    \n",
    "    df = pd.merge(all_book,all_trade, how='left', on=['stock_id','time_id'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# The root mean square percentage error\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return  np.round((np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))),3)\n",
    "\n",
    "# Transfer dataframe to submitting form\n",
    "\n",
    "def submit_format(df):\n",
    "    df['row_id'] = df['stock_id'].astype(str) + '-' + df['time_id'].astype(str)\n",
    "    df = df[['row_id','target']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb79418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traning X data\n",
    "\n",
    "df_train = train_and_test_data(list_order_book_file_train,list_order_trade_file_train)\n",
    "df_train_features = df_train.drop(['time_id'], axis=1)\n",
    "\n",
    "# Training y data\n",
    "\n",
    "df_train_target = pd.read_csv(f'{path}/train.csv')\n",
    "dtrain = lgb.Dataset(df_train_features, label=df_train_target['target'])\n",
    "\n",
    "# LightGBM Model\n",
    "\n",
    "para = {\n",
    "    'metric' : '',\n",
    "    'max_depth' : 30,\n",
    "    'num_leaves' : 100,\n",
    "    'learning_rate' : 0.05,\n",
    "    'feature_fraction' : 0.9,\n",
    "    'min_child_samples' : 19,\n",
    "    'min_child_weight' : 0.002,\n",
    "    'bagging_fraction' : 0.5,\n",
    "    'bagging_freq' : 1,\n",
    "    'reg_alpha' : 0.01,\n",
    "    'reg_lambda' : 4,\n",
    "    'cat_smooth' : 12,\n",
    "    'num_iterations' : 100,\n",
    "    'device':'gpu'\n",
    "}\n",
    "\n",
    "model = lgb.train(para, dtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63579c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# GridSearchCV for finding best parameters\n",
    "\n",
    "# parameters = {\n",
    "    \n",
    "# }\n",
    "\n",
    "# gbm = lgb.LGBMRegressor(\n",
    "#     metric = '',\n",
    "#     max_depth = 30,\n",
    "#     num_leaves = 100,\n",
    "#     learning_rate = 0.05,\n",
    "#     feature_fraction = 0.9,\n",
    "#     min_child_samples = 19,\n",
    "#     min_child_weight = 0.002,\n",
    "#     bagging_fraction = 0.5,\n",
    "#     bagging_freq = 1,\n",
    "#     reg_alpha = 0.01,\n",
    "#     reg_lambda = 4,\n",
    "#     cat_smooth = 12,\n",
    "#     num_iterations = 100,\n",
    "#     device = 'gpu'\n",
    "# )\n",
    "# gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='neg_median_absolute_error', cv=3, n_jobs = -1)\n",
    "# gsearch.fit(df_train_features.values, df_train_target['target'].values)\n",
    "# print(f'Best: {gsearch.best_score_} using {gsearch.best_params_}')\n",
    "# print('-----------------------------------------------------------')\n",
    "# print(gsearch.cv_results_['mean_test_score'])\n",
    "# print('-----------------------------------------------------------')\n",
    "# print(gsearch.cv_results_['params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddd6aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test X data\n",
    "\n",
    "df_test = train_and_test_data(list_order_book_file_test,list_order_trade_file_test)\n",
    "df_test_features = df_test.drop(['time_id'], axis=1)\n",
    "\n",
    "# Using LightGBM to predict test y\n",
    "\n",
    "ypred = model.predict(df_test_features)\n",
    "ypred = np.asarray(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529147fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate dataframe for submitting\n",
    "\n",
    "df_test['target'] = ypred\n",
    "df_test = df_test[['stock_id', 'time_id', 'target']]\n",
    "df_test = submit_format(df_test)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.columns = ['row_id','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids_jupyter",
   "language": "python",
   "name": "rapids-21.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
